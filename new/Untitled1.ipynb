{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d770b727-5d30-45dc-b813-e70491e75c1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Video 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244d175-f36f-403a-b661-149cb75e91d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ba0833-d6d5-47e4-bc0d-ee3e3c16b508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no errors\n"
     ]
    }
   ],
   "source": [
    "print(\"no errors\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89a3cb8f-c088-42c3-9dc9-8c4d00fbadf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python\n",
      "is\n",
      "such\n",
      "a\n",
      "trash\n",
      "language\n",
      ".\n",
      "I\n",
      "fucking\n",
      "hate\n",
      "it\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"Python is such a trash language . I fucking hate it \")\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    for word in sentence:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4b82c76-5758-46b5-b5e4-eee528dc6944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79700262-0087-4994-9075-e0fe1dcdf206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize , word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6892a15f-d7e1-44fd-95eb-672b57d8c4ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python is such a trash language .', 'I fucking hate it']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize('Python is such a trash language . I fucking hate it ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2646a924-b09a-429d-a5cd-af838c961f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python',\n",
       " 'is',\n",
       " 'such',\n",
       " 'a',\n",
       " 'trash',\n",
       " 'language',\n",
       " '.',\n",
       " 'I',\n",
       " 'fucking',\n",
       " 'hate',\n",
       " 'it']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize('Python is such a trash language . I fucking hate it ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49181003-d619-4390-a6bd-926230b06051",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Video 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f198ba2-0f5d-42f7-b4cd-643c53cee02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's the big deal in tokenization\n",
    "\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0f5b758-0937-4a20-8bcd-6ab25b780640",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "# blank english language component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e2e1cdb8-3065-46dd-ac19-5a86082ddd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python\n",
      "is\n",
      "such\n",
      "a\n",
      "trash\n",
      "'s\n",
      "language\n",
      "!\n",
      ".\n",
      "I\n",
      "fucking\n",
      "hate\n",
      "it\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Python is such a trash's language!. I fucking hate it\")\n",
    "for sentence in doc:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5206d20d-77a2-4138-ac67-41cbf3d14080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "नमस्ते False False\n",
      ", False False\n",
      "मेरा False False\n",
      "नाम False False\n",
      "आशीष False False\n",
      "है False False\n",
      "और False True\n",
      "मैं False False\n",
      "आपको False False\n",
      "खोजने False False\n",
      "की False False\n",
      "कोशिश False False\n",
      "कर False True\n",
      "रहा False False\n",
      "हूं False False\n",
      "। False False\n",
      "क्या False False\n",
      "आप False True\n",
      "एक False True\n",
      "हत्या False False\n",
      "में False False\n",
      "मेरी False False\n",
      "मदद False True\n",
      "करेंगे False False\n",
      "? False False\n"
     ]
    }
   ],
   "source": [
    "nlp =  spacy.blank(\"hi\")\n",
    "doc = nlp(\"नमस्ते, मेरा नाम आशीष है और मैं आपको खोजने की कोशिश कर रहा हूं। क्या आप एक हत्या में मेरी मदद करेंगे?\")\n",
    "for token in doc:\n",
    "    print(token , token.is_digit , token.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "285638ad-3f41-47ed-bace-8bcca378bd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e5803-100a-46e9-bf9b-be7c6dc3d9b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Video 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91428836-6d1d-4666-85b9-c342ac750d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b2db4fe-6a26-4b46-841d-619e46616201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a90e4e38-3bbc-4170-b61c-09ad35fd8ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain | PROPN | Captain\n",
      "america | PROPN | america\n",
      "ate | VERB | eat\n",
      "$ | SYM | $\n",
      "100 | NUM | 100\n",
      "of | ADP | of\n",
      "samosa | NOUN | samosa\n",
      ". | PUNCT | .\n",
      "Then | ADV | then\n",
      "he | PRON | he\n",
      "said | VERB | say\n",
      "do | AUX | do\n",
      "i | PRON | I\n",
      "have | VERB | have\n",
      "to | PART | to\n",
      "do | VERB | do\n",
      "this | PRON | this\n",
      "all | DET | all\n",
      "day | NOUN | day\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Captain america ate $100 of samosa. Then he said do i have to do this all day\")\n",
    "for token in doc:\n",
    "    print(token , \"|\" , token.pos_, \"|\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "693d39a5-d643-40a5-b639-97f5b3a69e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Captain america ate $\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    100\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " of samosa. Then he said do i have to do \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    this all day\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc,style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680bcda6-8e67-4b80-b11e-285d57ec2bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6045f34-3099-4249-a258-fea688903e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec48950b-dd7a-41f3-97ac-5453e54338fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0ea054c-06da-4543-bd05-16a103321184",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Video 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc929a52-4481-44a1-896b-f354a19a5b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why am i doing ths , idk \n",
    "# stemming and lemmitization\n",
    "import nltk \n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db52fb2a-cdd7-4a72-a651-f37996b9f986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple | appl\n",
      "ball | ball\n",
      "cat | cat\n",
      "levitating | levit\n",
      "ate | ate\n",
      "stalkfishing | stalkfish\n",
      "schooling | school\n",
      "adverb | adverb\n",
      "impossible | imposs\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"apple\",\"ball\",\"cat\",\"levitating\",\"ate\",\"stalkfishing\",\"schooling\",\"adverb\",\"impossible\"]\n",
    "for word in words:\n",
    "    print(word , \"|\" , stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af07b8f2-99b8-47f5-8781-1b71d0683c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |  \n",
      "apple | apple\n",
      "ball | ball\n",
      "cat | cat\n",
      "levitating | levitating\n",
      "ate | eat\n",
      "stalkfishing | stalkfishe\n",
      "schooling | school\n",
      "adverb | adverb\n",
      "impossible | impossible\n",
      "better | well\n",
      "best | good\n",
      "bro | bro\n",
      "brah | brah\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\" apple ball cat levitating ate stalkfishing schooling adverb impossible better best bro brah\")\n",
    "for word in doc:\n",
    "    print(word , \"|\", word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4ff1fb4-0209-4f42-be08-ad21df730004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "272916a3-9369-4813-828d-c65357f768df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = nlp.get_pipe('attribute_ruler')\n",
    "ar.add([[{\"TEXT\":\"Bro\"}] , [{\"TEXT\":\"Brah\"}]], {\"LEMMA\":\"brother\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d69a400b-a2c6-4fa0-adaf-61abcfb61da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |  \n",
      "apple | apple\n",
      "ball | ball\n",
      "cat | cat\n",
      "levitating | levitating\n",
      "ate | eat\n",
      "stalkfishing | stalkfishe\n",
      "schooling | school\n",
      "adverb | adverb\n",
      "impossible | impossible\n",
      "better | well\n",
      "best | good\n",
      "bro | bro\n",
      "brah | brah\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\" apple ball cat levitating ate stalkfishing schooling adverb impossible better best bro brah\")\n",
    "for word in doc:\n",
    "    print(word , \"|\", word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef67ead4-078b-4dda-9fea-9485a490665d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Video 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddb5b6ea-4f5c-4455-bee2-3ebca2e08d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon  |  PROPN | proper noun | NNP | noun, proper singular\n",
      "flew  |  VERB | verb | VBD | verb, past tense\n",
      "to  |  ADP | adposition | IN | conjunction, subordinating or preposition\n",
      "mars  |  NOUN | noun | NNS | noun, plural\n",
      "yesterday  |  NOUN | noun | NN | noun, singular or mass\n",
      ".  |  PUNCT | punctuation | . | punctuation mark, sentence closer\n",
      "He  |  PRON | pronoun | PRP | pronoun, personal\n",
      "carried  |  VERB | verb | VBD | verb, past tense\n",
      "masala  |  NOUN | noun | NN | noun, singular or mass\n",
      "dosa  |  NOUN | noun | NN | noun, singular or mass\n",
      "with  |  ADP | adposition | IN | conjunction, subordinating or preposition\n",
      "himself  |  PRON | pronoun | PRP | pronoun, personal\n"
     ]
    }
   ],
   "source": [
    "# PART OF SPEECH BS\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('Elon flew to mars yesterday . He carried masala dosa with himself')\n",
    "for token in doc:\n",
    "    print(token, \" | \" , token.pos_ , \"|\" , spacy.explain(token.pos_) , \"|\" , token.tag_ , \"|\" , spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0bd726c-f054-477d-9da9-c6a108a6d9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN | 1\n",
      "VERB | 2\n",
      "ADP | 2\n",
      "NOUN | 4\n",
      "PUNCT | 1\n",
      "PRON | 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count = doc.count_by(spacy.attrs.POS)\n",
    "for k,v in count.items():\n",
    "    print(doc.vocab[k].text , \"|\",v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b87dd19-33fc-4dfc-b5a4-20760b364651",
   "metadata": {},
   "source": [
    "## Video 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d0a4dab-8f83-4168-b87c-0988b219077b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NAME ENTITY RECOGNITION\n",
    "# NER\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2962fb69-946a-4c39-b8a3-c8c1aa3b91a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc= nlp(\"Tesla inc is going to boom this year but that has nothing to do with the famous tesla scientist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0483dd3-6768-4383-9c1b-4656edfe1a83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
